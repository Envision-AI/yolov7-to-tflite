{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolov7-Model-Conversion-Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHNsDxWpDBcIRVjnzw5DLr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saffie91/yolov7-tflite-conversion/blob/main/Yolov7_Model_Conversion_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xsLCUCQw_zk"
      },
      "outputs": [],
      "source": [
        "!python3 export.py --weights best.pt --grid --end2end --simplify --topk-all 100 --conf-thres 0.35 --img-size 320 320 --max-wh 320"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import coremltools\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "o9_AbSBOxM6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check model\n",
        "onnx_model = onnx.load(\"best.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "CNjSK-ULxNEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make session\n",
        "so = ort.SessionOptions()\n",
        "session = ort.InferenceSession('best.onnx')"
      ],
      "metadata": {
        "id": "YWqbnD3bxNHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the input\n",
        "def letterbox(im, new_shape=(320, 320), color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return im, r, (dw, dh)\n",
        "\n",
        "names = ['Glasses', 'Sunglasses', 'Beer', 'Ball', 'Pen','Piano', 'Headphones', 'Light switch', 'Footwear', 'Watch', 'Coffeemaker', 'Waste container', 'Window', 'Window blind', 'Door handle', \"Door\", \"Stairs\", 'Bicycle','Car','Motorcycle','Bus','Train','Truck','Traffic light','Fire hydrant','Bench','Bird','Cat','Dog','Backpack','Handbag','Suitcase','Bottle','Wine glass','Coffee cup','Fork','Knife','Spoon','Bowl','Chair','Couch','Plant','Bed','Table','Toilet','Television','Laptop','Computer mouse','Remote control','Computer keyboard','Mobile phone','Microwave oven','Oven','Toaster','Sink','Refrigerator','Book','Clock','Toothbrush']\n",
        "colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}\n",
        "\n",
        "image=cv2.imread('inference/images/bus.jpg')\n",
        "img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = img.copy()\n",
        "image, ratio, dwdh = letterbox(image, auto=False)\n",
        "image = image.transpose((2, 0, 1))\n",
        "image = np.expand_dims(image, 0)\n",
        "image = np.ascontiguousarray(image)\n",
        "\n",
        "im = image.astype(np.float32)\n",
        "im /= 255\n",
        "im.shape\n",
        "\n",
        "outname = [i.name for i in session.get_outputs()]\n",
        "outname\n",
        "\n",
        "inname = [i.name for i in session.get_inputs()]\n",
        "inname\n",
        "\n",
        "inp = {inname[0]:im}"
      ],
      "metadata": {
        "id": "YoUlyN4jxNJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(np.moveaxis(im[0], 0,2))"
      ],
      "metadata": {
        "id": "xc37SvOuxNOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time output\n",
        "start=time.time()\n",
        "outputs = session.run(outname, inp)[0]\n",
        "end=time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "id": "HeiafQ2E0ANC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check results\n",
        "ori_images = [img.copy()]\n",
        "\n",
        "for i,(batch_id,x0,y0,x1,y1,cls_id,score) in enumerate(outputs):\n",
        "    image = ori_images[int(batch_id)]\n",
        "    box = np.array([x0,y0,x1,y1])\n",
        "    box -= np.array(dwdh*2)\n",
        "    box /= ratio\n",
        "    box = box.round().astype(np.int32).tolist()\n",
        "    cls_id = int(cls_id)\n",
        "    score = round(float(score),3)\n",
        "    name = names[cls_id]\n",
        "    color = colors[name]\n",
        "    name += ' '+str(score)\n",
        "    cv2.rectangle(image,box[:2],box[2:],color,2)\n",
        "    cv2.putText(image,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[225, 255, 255],thickness=2)  \n",
        "\n",
        "Image.fromarray(ori_images[0])"
      ],
      "metadata": {
        "id": "pby262hb26RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to tf model\n",
        "\n",
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        " \n",
        "onnx_model = onnx.load(\"best.onnx\")\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph(\"best_tf2.pb\")"
      ],
      "metadata": {
        "id": "aK1Rx9AZ26Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check tf model\n",
        "model=tf.saved_model.load(\"best_tf2.pb\")\n",
        "infer = model.signatures[\"serving_default\"]\n",
        "print(infer.structured_outputs)"
      ],
      "metadata": {
        "id": "gIHOAIJF26WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time tf model\n",
        "start=time.time()\n",
        "labeling = infer(tf.constant(im))['output']\n",
        "end=time.time()\n",
        "print(end-start)\n",
        "print(\"Result after saving and loading:\\n\", labeling)"
      ],
      "metadata": {
        "id": "wM26gCwj26Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the output\n",
        "ori_images = [img.copy()]\n",
        "\n",
        "for i,(batch_id,x0,y0,x1,y1,cls_id,score) in enumerate(labeling):\n",
        "    image = ori_images[int(batch_id)]\n",
        "    box = np.array([x0,y0,x1,y1])\n",
        "    box -= np.array(dwdh*2)\n",
        "    box /= ratio\n",
        "    box = box.round().astype(np.int32).tolist()\n",
        "    cls_id = int(cls_id)\n",
        "    score = round(float(score),3)\n",
        "    name = names[cls_id]\n",
        "    color = colors[name]\n",
        "    name += ' '+str(score)\n",
        "    cv2.rectangle(image,box[:2],box[2:],color,2)\n",
        "    cv2.putText(image,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[225, 255, 255],thickness=2)  \n",
        "\n",
        "Image.fromarray(ori_images[0])"
      ],
      "metadata": {
        "id": "7dWSApiOArXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadImages:\n",
        "    # YOLOv5 image/video dataloader, i.e. `python detect.py --source image.jpg/vid.mp4`\n",
        "    def __init__(self, path, img_size=640, stride=32, auto=True):\n",
        "        files = []\n",
        "        for p in sorted(path) if isinstance(path, (list, tuple)) else [path]:\n",
        "            p = str(Path(p).resolve())\n",
        "            if '*' in p:\n",
        "                files.extend(sorted(glob.glob(p, recursive=True)))  # glob\n",
        "            elif os.path.isdir(p):\n",
        "                files.extend(sorted(glob.glob(os.path.join(p, '*.*'))))  # dir\n",
        "            elif os.path.isfile(p):\n",
        "                files.append(p)  # files\n",
        "            else:\n",
        "                raise FileNotFoundError(f'{p} does not exist')\n",
        "\n",
        "        images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]\n",
        "        videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]\n",
        "        ni, nv = len(images), len(videos)\n",
        "        \n",
        "        self.img_size = img_size\n",
        "        self.stride = stride\n",
        "        self.files = images + videos\n",
        "        self.nf = ni + nv  # number of files\n",
        "        self.video_flag = [False] * ni\n",
        "        self.mode = 'image'\n",
        "        self.auto = auto\n",
        "        if any(videos):\n",
        "            self.new_video(videos[0])  # new video\n",
        "        else:\n",
        "            self.cap = None\n",
        "        assert self.nf > 0, f'No images or videos found in {p}. ' \\\n",
        "                            f'Supported formats are:\\nimages: {IMG_FORMATS}\\nvideos: {VID_FORMATS}'\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.count == self.nf:\n",
        "            raise StopIteration\n",
        "        path = self.files[self.count]\n",
        "\n",
        "        if self.video_flag[self.count]:\n",
        "            # Read video\n",
        "            self.mode = 'video'\n",
        "            ret_val, img0 = self.cap.read()\n",
        "            while not ret_val:\n",
        "                self.count += 1\n",
        "                self.cap.release()\n",
        "                if self.count == self.nf:  # last video\n",
        "                    raise StopIteration\n",
        "                path = self.files[self.count]\n",
        "                self.new_video(path)\n",
        "                ret_val, img0 = self.cap.read()\n",
        "\n",
        "            self.frame += 1\n",
        "            s = f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: '\n",
        "\n",
        "        else:\n",
        "            # Read image\n",
        "            self.count += 1\n",
        "            img0 = cv2.imread(path)  # BGR\n",
        "            assert img0 is not None, f'Image Not Found {path}'\n",
        "            s = f'image {self.count}/{self.nf} {path}: '\n",
        "\n",
        "        # Padded resize\n",
        "        img = letterbox(img0, self.img_size, stride=self.stride, auto=self.auto)[0]\n",
        "\n",
        "        # Convert\n",
        "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return path, img, img0, self.cap, s\n",
        "\n",
        "    def new_video(self, path):\n",
        "        self.frame = 0\n",
        "        self.cap = cv2.VideoCapture(path)\n",
        "        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nf  # number of files"
      ],
      "metadata": {
        "id": "xgWUUF7yA2Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#need representative dataset for quantization\n",
        "def representative_dataset_gen(dataset, ncalib=100):\n",
        "    # Representative dataset generator for use with converter.representative_dataset, returns a generator of np arrays\n",
        "    for n, (path, img, im0s, vid_cap, string) in enumerate(dataset):\n",
        "        # im = np.transpose(img, [1, 2, 0])\n",
        "        im=img\n",
        "        im = np.expand_dims(im, axis=0).astype(np.float32)\n",
        "        im /= 255\n",
        "        yield [im]\n",
        "        if n >= ncalib:\n",
        "            break"
      ],
      "metadata": {
        "id": "qy_a7LR4A399"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgsz=320\n",
        "data='data/coco-Copy1.yaml'\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"best_tf2.pb\")\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# dataset = LoadImages(check_dataset(check_yaml(data))['train'], img_size=imgsz, auto=False)\n",
        "converter.representative_dataset = lambda: representative_dataset_gen(im[0])\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.target_spec.supported_types = []\n",
        "converter.inference_input_type = tf.int8  # or tf.int8\n",
        "converter.inference_output_type = tf.int8  # or tf.int8\n",
        "converter.experimental_new_quantizer = True\n",
        "converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n",
        "tflite_model = converter.convert()\n",
        "open('best-tflite-int8-2.tflite', \"wb\").write(tflite_model)"
      ],
      "metadata": {
        "id": "Kc3ol2wgA4Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tflite model try\n",
        "tflite_model='best-tflite3.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model)\n",
        "interpreter.allocate_tensors()\n"
      ],
      "metadata": {
        "id": "Z4QtidIWBhFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image=cv2.imread('bus.jpg')\n",
        "# img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "# image = img.copy()\n",
        "# image, ratio, dwdh = letterbox(image, auto=False)\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "interpreter.set_tensor(input_details[0]['index'], im)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Inference output is {}\".format(output_data))"
      ],
      "metadata": {
        "id": "v0lYBnT3BnWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "ori_images = [img.copy()]\n",
        "names = ['Glasses', 'Sunglasses', 'Beer', 'Ball', 'Pen','Piano', 'Headphones', 'Light switch', 'Footwear', 'Watch', 'Coffeemaker', 'Waste container', 'Window', 'Window blind', 'Door handle', \"Door\", \"Stairs\", 'Bicycle','Car','Motorcycle','Bus','Train','Truck','Traffic light','Fire hydrant','Bench','Bird','Cat','Dog','Backpack','Handbag','Suitcase','Bottle','Wine glass','Coffee cup','Fork','Knife','Spoon','Bowl','Chair','Couch','Plant','Bed','Table','Toilet','Television','Laptop','Computer mouse','Remote control','Computer keyboard','Mobile phone','Microwave oven','Oven','Toaster','Sink','Refrigerator','Book','Clock','Toothbrush']\n",
        "colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}\n",
        "for i,(batch_id,x0,y0,x1,y1,cls_id,score) in enumerate(output_data):\n",
        "    image = ori_images[0]\n",
        "    box = np.array([x0,y0,x1,y1])\n",
        "    box -= np.array(dwdh*2)\n",
        "    box /= ratio\n",
        "    box = box.round().astype(np.int32).tolist()\n",
        "    cls_id = int(cls_id)\n",
        "    score = round(float(score),3)\n",
        "    name = names[cls_id]\n",
        "    color = colors[name]\n",
        "    name += ' '+str(score)\n",
        "    cv2.rectangle(image,box[:2],box[2:],color,2)\n",
        "    cv2.putText(image,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[225, 255, 255],thickness=2)  \n",
        "Image.fromarray(ori_images[0])"
      ],
      "metadata": {
        "id": "T3RZgv-FBn9y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}