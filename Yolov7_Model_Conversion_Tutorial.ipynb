{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yolov7-Model-Conversion-Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOe/xNx6J+wEaHEpY5z0Fzq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saffie91/yolov7-tflite-conversion/blob/main/Yolov7_Model_Conversion_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YoloV7 TFlite Conversion\n",
        "\n",
        "First of all we're going to need to export the onnx from the YoloV7 repo export.py using:"
      ],
      "metadata": {
        "id": "Td8l6yMMaAC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 export.py --weights best.pt --grid --end2end --simplify --topk-all 100 --conf-thres 0.35 --img-size 320 320 --max-wh 320"
      ],
      "metadata": {
        "id": "aEzCbaDLaDo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure that you have the right versions of these libraries. Tf-nightly is required to convert the model to tflite properly."
      ],
      "metadata": {
        "id": "-vVsV-SuayMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import coremltools\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "o9_AbSBOxM6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the onnx model to see if there were any issues with the export. If the model passes the checker make an inference session."
      ],
      "metadata": {
        "id": "ZFzihusda8Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check model\n",
        "onnx_model = onnx.load(\"best.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "CNjSK-ULxNEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make session\n",
        "so = ort.SessionOptions()\n",
        "session = ort.InferenceSession('best.onnx')"
      ],
      "metadata": {
        "id": "YWqbnD3bxNHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to make sure that the input is right. We will be resizing every image to 320 since thats what we want to use for our TFlite image size.\n",
        "\n",
        "Put the names of the classes for your model here.\n",
        "\n",
        "When the resizing is being done make sure that you are keeping the aspect ratio by using padding.\n",
        "\n",
        "Rescale your image and get your input ready for the Onnx model"
      ],
      "metadata": {
        "id": "FiJ33pTVbiHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the input\n",
        "def letterbox(im, new_shape=(320, 320), color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = im.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return im, r, (dw, dh)\n",
        "\n",
        "names = []\n",
        "colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}\n",
        "\n",
        "image=cv2.imread('inference/images/bus.jpg')\n",
        "img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = img.copy()\n",
        "image, ratio, dwdh = letterbox(image, auto=False)\n",
        "image = image.transpose((2, 0, 1))\n",
        "image = np.expand_dims(image, 0)\n",
        "image = np.ascontiguousarray(image)\n",
        "\n",
        "im = image.astype(np.float32)\n",
        "im /= 255\n",
        "im.shape\n",
        "\n",
        "outname = [i.name for i in session.get_outputs()]\n",
        "outname\n",
        "\n",
        "inname = [i.name for i in session.get_inputs()]\n",
        "inname\n",
        "\n",
        "inp = {inname[0]:im}"
      ],
      "metadata": {
        "id": "YoUlyN4jxNJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check your image to see if the input is right"
      ],
      "metadata": {
        "id": "t7OZIsXEdBlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(im.shape)\n",
        "plt.imshow(np.moveaxis(im[0], 0,2))"
      ],
      "metadata": {
        "id": "xc37SvOuxNOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the inference on the onnx model to see if everything looks good. You want to focus on the prediction scores to see if anything changed."
      ],
      "metadata": {
        "id": "MGCODvV1dGq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#time output\n",
        "start=time.time()\n",
        "outputs = session.run(outname, inp)[0]\n",
        "end=time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "id": "HeiafQ2E0ANC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check results\n",
        "ori_images = [img.copy()]\n",
        "\n",
        "for i,(batch_id,x0,y0,x1,y1,cls_id,score) in enumerate(outputs):\n",
        "    image = ori_images[int(batch_id)]\n",
        "    box = np.array([x0,y0,x1,y1])\n",
        "    box -= np.array(dwdh*2)\n",
        "    box /= ratio\n",
        "    box = box.round().astype(np.int32).tolist()\n",
        "    cls_id = int(cls_id)\n",
        "    score = round(float(score),3)\n",
        "    name = names[cls_id]\n",
        "    color = colors[name]\n",
        "    name += ' '+str(score)\n",
        "    cv2.rectangle(image,box[:2],box[2:],color,2)\n",
        "    cv2.putText(image,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[225, 255, 255],thickness=2)  \n",
        "\n",
        "Image.fromarray(ori_images[0])"
      ],
      "metadata": {
        "id": "pby262hb26RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the Onnx model seems to be fine, its time to convert to TF model (.pb) This can easily be done by using onnx_tf library."
      ],
      "metadata": {
        "id": "W-ELW7t1ekow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to tf model\n",
        "\n",
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        " \n",
        "onnx_model = onnx.load(\"best.onnx\")\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph(\"best_tf2.pb\")"
      ],
      "metadata": {
        "id": "aK1Rx9AZ26Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to before, we check the TF model to see if the same image being ran through it is given the same results. If there are new false positives or the scores seem off you might have had an issue converting this model."
      ],
      "metadata": {
        "id": "RHqEiw_WfAJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check tf model\n",
        "model=tf.saved_model.load(\"best_tf2.pb\")\n",
        "infer = model.signatures[\"serving_default\"]\n",
        "print(infer.structured_outputs)"
      ],
      "metadata": {
        "id": "gIHOAIJF26WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time tf model\n",
        "start=time.time()\n",
        "labeling = infer(tf.constant(im))['output']\n",
        "end=time.time()\n",
        "print(end-start)\n",
        "print(\"Result after saving and loading:\\n\", labeling)"
      ],
      "metadata": {
        "id": "wM26gCwj26Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the output\n",
        "ori_images = [img.copy()]\n",
        "\n",
        "for i,(batch_id,x0,y0,x1,y1,cls_id,score) in enumerate(labeling):\n",
        "    image = ori_images[int(batch_id)]\n",
        "    box = np.array([x0,y0,x1,y1])\n",
        "    box -= np.array(dwdh*2)\n",
        "    box /= ratio\n",
        "    box = box.round().astype(np.int32).tolist()\n",
        "    cls_id = int(cls_id)\n",
        "    score = round(float(score),3)\n",
        "    name = names[cls_id]\n",
        "    color = colors[name]\n",
        "    name += ' '+str(score)\n",
        "    cv2.rectangle(image,box[:2],box[2:],color,2)\n",
        "    cv2.putText(image,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[225, 255, 255],thickness=2)  \n",
        "\n",
        "Image.fromarray(ori_images[0])"
      ],
      "metadata": {
        "id": "7dWSApiOArXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to convert this tf model into tflite. If you will use the fp16 inputs its easier. However if you want to quantize this model to int8, you will need to make a representative dataset generator to feed images to."
      ],
      "metadata": {
        "id": "Lvi83K3SfSpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#need representative dataset for quantization\n",
        "def representative_dataset_gen(dataset, ncalib=100):\n",
        "    # Representative dataset generator for use with converter.representative_dataset, returns a generator of np arrays\n",
        "    for n, (path, img, im0s, vid_cap, string) in enumerate(dataset):\n",
        "        # im = np.transpose(img, [1, 2, 0])\n",
        "        im=img\n",
        "        im = np.expand_dims(im, axis=0).astype(np.float32)\n",
        "        im /= 255\n",
        "        yield [im]\n",
        "        if n >= ncalib:\n",
        "            break"
      ],
      "metadata": {
        "id": "qy_a7LR4A399"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgsz=320\n",
        "int8=False\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(\"best_tf2.pb\")\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "if int8:\n",
        "  converter.representative_dataset = lambda: representative_dataset_gen(im[0])\n",
        "  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "  converter.target_spec.supported_types = []\n",
        "  converter.inference_input_type = tf.int8  # or tf.int8\n",
        "  converter.inference_output_type = tf.int8  # or tf.int8\n",
        "  converter.experimental_new_quantizer = True\n",
        "#adding NMS\n",
        "converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n",
        "tflite_model = converter.convert()\n",
        "open('best-tflite.tflite', \"wb\").write(tflite_model)"
      ],
      "metadata": {
        "id": "Kc3ol2wgA4Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if the model has converted properly. Try without int8 quantization first"
      ],
      "metadata": {
        "id": "wgDaKZ-oglMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tflite model try\n",
        "tflite_model='best-tflite.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model)\n",
        "interpreter.allocate_tensors()\n"
      ],
      "metadata": {
        "id": "Z4QtidIWBhFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "interpreter.set_tensor(input_details[0]['index'], im)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"Inference output is {}\".format(output_data))"
      ],
      "metadata": {
        "id": "v0lYBnT3BnWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "ori_images = [img.copy()]\n",
        "names = ['Glasses', 'Sunglasses', 'Beer', 'Ball', 'Pen','Piano', 'Headphones', 'Light switch', 'Footwear', 'Watch', 'Coffeemaker', 'Waste container', 'Window', 'Window blind', 'Door handle', \"Door\", \"Stairs\", 'Bicycle','Car','Motorcycle','Bus','Train','Truck','Traffic light','Fire hydrant','Bench','Bird','Cat','Dog','Backpack','Handbag','Suitcase','Bottle','Wine glass','Coffee cup','Fork','Knife','Spoon','Bowl','Chair','Couch','Plant','Bed','Table','Toilet','Television','Laptop','Computer mouse','Remote control','Computer keyboard','Mobile phone','Microwave oven','Oven','Toaster','Sink','Refrigerator','Book','Clock','Toothbrush']\n",
        "colors = {name:[random.randint(0, 255) for _ in range(3)] for i,name in enumerate(names)}\n",
        "for i,(batch_id,x0,y0,x1,y1,cls_id,score) in enumerate(output_data):\n",
        "    image = ori_images[0]\n",
        "    box = np.array([x0,y0,x1,y1])\n",
        "    box -= np.array(dwdh*2)\n",
        "    box /= ratio\n",
        "    box = box.round().astype(np.int32).tolist()\n",
        "    cls_id = int(cls_id)\n",
        "    score = round(float(score),3)\n",
        "    name = names[cls_id]\n",
        "    color = colors[name]\n",
        "    name += ' '+str(score)\n",
        "    cv2.rectangle(image,box[:2],box[2:],color,2)\n",
        "    cv2.putText(image,name,(box[0], box[1] - 2),cv2.FONT_HERSHEY_SIMPLEX,0.75,[225, 255, 255],thickness=2)  \n",
        "Image.fromarray(ori_images[0])"
      ],
      "metadata": {
        "id": "T3RZgv-FBn9y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}